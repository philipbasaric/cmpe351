{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03291531-99d9-40cb-8c98-b089ca4d573c",
   "metadata": {},
   "source": [
    "### Method 1: Model Averaging\n",
    "Model averaging is an ensemble technique where the predictions of multiple models are combined to create a single prediction. In model averaging, the weights assigned to each model are typically determined by their performance on a validation dataset. The predictions of the models are then weighted by these values, and the resulting weighted average is used as the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371d6577-a0d8-4969-b2a3-de1fec753922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Train a random forest classifier\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set using each base model\n",
    "preds1 = lr_model.predict(X_val)\n",
    "preds2 = rf_model.predict(X_val)\n",
    "\n",
    "# Calculate the accuracy of each base model on the validation set\n",
    "acc1 = np.mean(preds1 == y_val)\n",
    "acc2 = np.mean(preds2 == y_val)\n",
    "\n",
    "# Determine the weight for each model based on its validation set accuracy\n",
    "weights = [acc1 / (acc1 + acc2), acc2 / (acc1 + acc2)]\n",
    "\n",
    "# Combine the predictions of each base model using the weights determined above\n",
    "ensemble_preds = (weights[0] * preds1) + (weights[1] * preds2)\n",
    "\n",
    "# Evaluate the performance of the ensemble model on a test set\n",
    "ensemble_acc = np.mean(ensemble_preds.round() == y_test)\n",
    "\n",
    "print(ensemble_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1088b2-a290-4400-b257-7e8a8c307380",
   "metadata": {},
   "source": [
    "### Method 2: Voting\n",
    "The voting classifier works by taking the predictions from each model and using a majority vote to determine the final prediction. This can be done either by 'hard voting' (where the class with the most votes is selected) or 'soft voting' (where the predicted probabilities for each class are averaged across the models, and the class with the highest average probability is selected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3398e67f-1e14-4cc4-b79a-4b585738e112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Generate a synthetic classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define your base models\n",
    "model1 = RandomForestClassifier(n_estimators=100)\n",
    "model2 = SVC(kernel='rbf', probability=True)\n",
    "\n",
    "# Create an ensemble model using the VotingClassifier\n",
    "ensemble = VotingClassifier(estimators=[('rf', model1), ('svm', model2)], voting='soft')\n",
    "\n",
    "# Train the ensemble model on your dataset\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "ensemble.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d5b294-4eac-4de3-aac8-0507beba2460",
   "metadata": {},
   "source": [
    "### Method 3: Stacking\n",
    "The idea behind stacking is to train several models on the same dataset and use their predictions as input to a higher-level model, which then makes the final prediction. The base models predictions are used as features for the meta-model. Once the meta-model is trained, it can be used to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7529c3ab-ff9f-4fdd-b634-9751c6ea630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Create a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define two base models\n",
    "base_model_1 = RandomForestClassifier(random_state=42)\n",
    "base_model_2 = LogisticRegression(random_state=42)\n",
    "\n",
    "# Define a meta model\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "\n",
    "class StackingModel:\n",
    "    \n",
    "    def __init__(self, base_models, meta_model):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        base_model_preds = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            model.fit(X, y)\n",
    "            y_pred = model.predict(X)\n",
    "            base_model_preds[:, i] = y_pred\n",
    "        self.meta_model.fit(base_model_preds, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        base_model_preds = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            y_pred = model.predict(X)\n",
    "            base_model_preds[:, i] = y_pred\n",
    "        return self.meta_model.predict(base_model_preds)\n",
    "\n",
    "# Create the stacking model\n",
    "stacking_model = StackingModel([base_model_1, base_model_2], meta_model)\n",
    "\n",
    "# Train the stacking model on the training data\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the stacking model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
